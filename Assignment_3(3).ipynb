{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8JZdEZ9xu26BaE19q4XcC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/90485462/ISOM676/blob/main/Assignment_3(3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiJ4URnSqX1n",
        "outputId": "a23c7f85-5afd-4b51-acc4-13c9a37ab9d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.3/spark-3.2.3-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.2.3-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.3-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Fu-wxozuko1",
        "outputId": "c3b32438-d5b2-44f7-c4b0-4b1346988b27"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Dataset"
      ],
      "metadata": {
        "id": "yxypsbShD5Sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/spambase.data'  # Update this path\n",
        "\n",
        "# If using pandas and want to convert to Spark DataFrame later\n",
        "import pandas as pd\n",
        "df_pd = pd.read_csv(file_path, header=None)"
      ],
      "metadata": {
        "id": "NdB3hc4JyPgS"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize PySpark Session"
      ],
      "metadata": {
        "id": "NXxLd3VMD4Fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"Spam Detection\").getOrCreate()\n",
        "\n",
        "# Convert pandas DataFrame to Spark DataFrame\n",
        "df = spark.createDataFrame(df_pd)\n",
        "\n",
        "# Alternatively, load directly into Spark DataFrame if schema is known\n",
        "# Example schema definition if needed\n",
        "from pyspark.sql.types import StructType, StructField, FloatType\n",
        "\n",
        "schema = StructType([\n",
        "    *(StructField(f\"feature_{i}\", FloatType(), False) for i in range(57)),\n",
        "    StructField(\"label\", FloatType(), False)\n",
        "])\n",
        "\n",
        "df = spark.read.csv(file_path, schema=schema, header=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14RdKPg3D0CQ",
        "outputId": "61897592-68ff-469e-d6d5-fc98d7dffc00"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:371: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
            "  for column, series in pdf.iteritems():\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the Data"
      ],
      "metadata": {
        "id": "hYE5C4BjD8EQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(inputCols=df.columns[:-1], outputCol=\"features\")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Split the data\n",
        "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)"
      ],
      "metadata": {
        "id": "8kA9BhSKrFnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Models"
      ],
      "metadata": {
        "id": "2fGgtYCbHTSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
        "rf_model = rf.fit(train_df)\n",
        "rf_predictions = rf_model.transform(test_df)"
      ],
      "metadata": {
        "id": "OhL1JR57ED08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "\n",
        "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
        "gbt_model = gbt.fit(train_df)\n",
        "gbt_predictions = gbt_model.transform(test_df)"
      ],
      "metadata": {
        "id": "fMKGjgfMriV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "\n",
        "svm = LinearSVC(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
        "svm_model = svm.fit(train_df)\n",
        "svm_predictions = svm_model.transform(test_df)"
      ],
      "metadata": {
        "id": "OAkYuyJ7HI69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
        "dt_model = dt.fit(train_df)\n",
        "dt_predictions = dt_model.transform(test_df)"
      ],
      "metadata": {
        "id": "j9EnKjrtktYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
        "lr_model = lr.fit(train_df)\n",
        "lr_predictions = lr_model.transform(test_df)"
      ],
      "metadata": {
        "id": "EgHXdo2vkz2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating Cost"
      ],
      "metadata": {
        "id": "iHgRM8JdtNn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Average Cost\n",
        "def calculate_average_cost(predictions, cost_fp=1, cost_fn=10):\n",
        "    # Calculate the costs based on the condition: FP and FN\n",
        "    predictions = predictions.withColumn('cost',\n",
        "                                         when((predictions['label'] == 0) & (predictions['prediction'] == 1), cost_fp)\n",
        "                                         .when((predictions['label'] == 1) & (predictions['prediction'] == 0), cost_fn)\n",
        "                                         .otherwise(0))\n",
        "    # Sum the total cost\n",
        "    total_cost = predictions.groupBy().sum('cost').collect()[0][0]\n",
        "\n",
        "    # Count the number of instances evaluated\n",
        "    num_instances = predictions.count()\n",
        "\n",
        "    # Calculate the average cost per instance\n",
        "    average_cost = total_cost / num_instances if num_instances else 0\n",
        "    return average_cost"
      ],
      "metadata": {
        "id": "SEAgroQmxQBG"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_average_cost = calculate_average_cost(rf_predictions)\n",
        "gbt_average_cost = calculate_average_cost(gbt_predictions)\n",
        "svm_average_cost = calculate_average_cost(svm_predictions)\n",
        "dt_average_cost = calculate_average_cost(dt_predictions)\n",
        "lr_average_cost = calculate_average_cost(lr_predictions)\n",
        "\n",
        "print(f\"Random Forest Average Cost: {rf_average_cost}\")\n",
        "print(f\"Gradient Boosted Trees Average Cost: {gbt_average_cost}\")\n",
        "print(f\"SVM Average Cost: {svm_average_cost}\")\n",
        "print(f\"Decision Tree Average Cost: {dt_average_cost}\")\n",
        "print(f\"Logistic Regression Average Cost: {lr_average_cost}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb_OLn4SxSg8",
        "outputId": "d66c2541-8d67-489b-d90e-437954d786d6"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Average Cost: 0.610730593607306\n",
            "Gradient Boosted Trees Average Cost: 0.4954337899543379\n",
            "SVM Average Cost: 0.4246575342465753\n",
            "Decision Tree Average Cost: 0.680365296803653\n",
            "Logistic Regression Average Cost: 0.4885844748858447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Assuming df is your Spark DataFrame\n",
        "\n",
        "# Feature Scaling\n",
        "assembler = VectorAssembler(inputCols=df.columns[:-1], outputCol=\"assembled_features\")\n",
        "scaler = StandardScaler(inputCol=\"assembled_features\", outputCol=\"features\", withStd=True, withMean=False)\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression(labelCol=\"label\", maxIter=10)\n",
        "\n",
        "# Support Vector Machine\n",
        "svm = LinearSVC(labelCol=\"label\", maxIter=10)\n",
        "\n",
        "# Pipeline for LR and SVM (you can run them separately)\n",
        "pipeline_lr = Pipeline(stages=[assembler, scaler, lr])\n",
        "pipeline_svm = Pipeline(stages=[assembler, scaler, svm])\n",
        "\n",
        "# ParamGrid for hyperparameter tuning\n",
        "paramGrid_lr = ParamGridBuilder() \\\n",
        "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
        "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
        "    .build()\n",
        "\n",
        "paramGrid_svm = ParamGridBuilder() \\\n",
        "    .addGrid(svm.regParam, [0.01, 0.1, 1.0]) \\\n",
        "    .build()\n",
        "\n",
        "# Evaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
        "\n",
        "# CrossValidator\n",
        "cv_lr = CrossValidator(estimator=pipeline_lr,\n",
        "                       estimatorParamMaps=paramGrid_lr,\n",
        "                       evaluator=evaluator,\n",
        "                       numFolds=5)\n",
        "\n",
        "cv_svm = CrossValidator(estimator=pipeline_svm,\n",
        "                        estimatorParamMaps=paramGrid_svm,\n",
        "                        evaluator=evaluator,\n",
        "                        numFolds=5)\n",
        "\n",
        "# Fit models\n",
        "cv_model_lr = cv_lr.fit(df)\n",
        "cv_model_svm = cv_svm.fit(df)\n",
        "\n",
        "# Best models\n",
        "best_model_lr = cv_model_lr.bestModel\n",
        "best_model_svm = cv_model_svm.bestModel"
      ],
      "metadata": {
        "id": "VLXLC-O-QcCl"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Prediction\n",
        "best_predictions_lr = best_model_lr.stages[-1].transform(test_df)\n",
        "best_predictions_svm = best_model_svm.stages[-1].transform(test_df)"
      ],
      "metadata": {
        "id": "eabPoYE5RgQy"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b_lr_average_cost = calculate_average_cost(best_predictions_lr)\n",
        "b_svm_average_cost = calculate_average_cost(best_predictions_svm)\n",
        "\n",
        "print(f\"Logistic Regression Average Cost: {b_lr_average_cost}\")\n",
        "print(f\"SVM Cost: {b_svm_average_cost}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDQSQUn7R_mQ",
        "outputId": "799cdb41-4ea6-49c0-8a10-c57f58f5d939"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Average Cost: 0.5125570776255708\n",
            "SVM Cost: 0.5057077625570776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Selection"
      ],
      "metadata": {
        "id": "5mhgDS1mWHYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.stat import Correlation\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Assemble features into a vector\n",
        "assembler = VectorAssembler(inputCols=df.columns[:-1], outputCol=\"features\")\n",
        "vector_df = assembler.transform(df)\n",
        "\n",
        "# Compute Pearson correlation matrix\n",
        "correlation_matrix = Correlation.corr(vector_df, \"features\").head()\n",
        "\n",
        "# Print the correlation matrix\n",
        "print(str(correlation_matrix[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaPOjBE4WJcN",
        "outputId": "ba7c25c4-1087-4685-8c6f-8d7af458e65d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DenseMatrix([[ 1.00000000e+00, -1.67594660e-02,  6.56267675e-02, ...,\n",
            "               4.44908620e-02,  6.13824002e-02,  8.91647777e-02],\n",
            "             [-1.67594660e-02,  1.00000000e+00, -3.35256801e-02, ...,\n",
            "               2.08284235e-03,  2.71204540e-04, -2.26796971e-02],\n",
            "             [ 6.56267675e-02, -3.35256801e-02,  1.00000000e+00, ...,\n",
            "               9.73980424e-02,  1.07462734e-01,  7.01135859e-02],\n",
            "             ...,\n",
            "             [ 4.44908620e-02,  2.08284235e-03,  9.73980424e-02, ...,\n",
            "               1.00000000e+00,  4.92638297e-01,  1.62313609e-01],\n",
            "             [ 6.13824002e-02,  2.71204540e-04,  1.07462734e-01, ...,\n",
            "               4.92638297e-01,  1.00000000e+00,  4.75485956e-01],\n",
            "             [ 8.91647777e-02, -2.26796971e-02,  7.01135859e-02, ...,\n",
            "               1.62313609e-01,  4.75485956e-01,  1.00000000e+00]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlujvt6DZe1N",
        "outputId": "f6e388e5-07cc-4ea8-ddfd-f267e868b11a"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(pearson(features)=DenseMatrix(57, 57, [1.0, -0.0168, 0.0656, 0.0133, 0.0231, 0.0597, 0.0077, -0.0039, ..., 0.1122, 0.006, 0.0363, 0.2019, 0.0426, 0.1623, 0.4755, 1.0], False))"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.stat import Correlation\n",
        "import numpy as np\n",
        "\n",
        "# Assuming `correlation_matrix` is your DenseMatrix object inside a Row as shown\n",
        "dense_matrix = correlation_matrix['pearson(features)']  # Extracting the DenseMatrix\n",
        "\n",
        "# Convert DenseMatrix to a NumPy array\n",
        "corr_matrix = np.array(dense_matrix.toArray())"
      ],
      "metadata": {
        "id": "Do5GDdIvcLCd"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a threshold for high correlation\n",
        "threshold = 0.9\n",
        "\n",
        "# Assuming you have a list of feature names corresponding to the columns of the correlation matrix\n",
        "feature_names = [\"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\",\n",
        "    \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \"word_freq_order\", \"word_freq_mail\",\n",
        "    \"word_freq_receive\", \"word_freq_will\", \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n",
        "    \"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\n",
        "    \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\",\n",
        "    \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\",\n",
        "    \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \"word_freq_415\", \"word_freq_85\",\n",
        "    \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\", \"word_freq_direct\",\n",
        "    \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\", \"word_freq_re\",\n",
        "    \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\", \"char_freq_(\",\n",
        "    \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_#\", \"capital_run_length_average\",\n",
        "    \"capital_run_length_longest\", \"capital_run_length_total\", \"class_label\"]  # Adjust this list to match your actual features\n",
        "\n",
        "# Identify pairs of highly correlated features\n",
        "highly_correlated_pairs = []\n",
        "\n",
        "for i in range(len(corr_matrix)):\n",
        "    for j in range(i+1, len(corr_matrix)):\n",
        "        if abs(corr_matrix[i, j]) > threshold:\n",
        "            highly_correlated_pairs.append((feature_names[i], feature_names[j]))\n",
        "\n",
        "# Print the highly correlated pairs\n",
        "for pair in highly_correlated_pairs:\n",
        "    print(f\"Highly correlated pair: {pair}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0yaeQHscXHQ",
        "outputId": "4d9d5438-c4a8-4185-94e7-d74c3291b2b8"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highly correlated pair: ('word_freq_857', 'word_freq_415')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you decide to drop 'word_freq_415' and keep 'word_freq_857'\n",
        "# Remove 'word_freq_415' from the feature list\n",
        "selected_features = [feature for feature in df.columns if feature != 'word_freq_415']\n",
        "df_transformed = assembler.transform(df)"
      ],
      "metadata": {
        "id": "8Pzk_hcgdTdu"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input columns for the features\n",
        "feature_columns = df_transformed.columns[:-1]\n",
        "\n",
        "# Assemble the feature columns into a single vector\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "\n",
        "# Optionally, split the data into training and test sets\n",
        "(train_data, test_data) = df_transformed.randomSplit([0.8, 0.2], seed=41)"
      ],
      "metadata": {
        "id": "UiamJfIpd101"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=20)\n",
        "\n",
        "# Train the model\n",
        "model = gbt.fit(df_transformed)"
      ],
      "metadata": {
        "id": "oyaCtuIqd9-E"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.transform(test_data)"
      ],
      "metadata": {
        "id": "bB_C26EDeXr1"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_cost = calculate_average_cost(predictions)\n",
        "print(f\"Average Cost: {average_cost}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IHYeMougMZU",
        "outputId": "7db1b7b3-6f8e-48f9-bf41-94abc38af703"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Cost: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = BinaryClassificationEvaluator()\n",
        "\n",
        "# You can specify the metric name in the evaluator (e.g., areaUnderROC or areaUnderPR)\n",
        "roc_auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
        "\n",
        "# Print the evaluation result\n",
        "print(f\"ROC AUC: {roc_auc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBTVeLGegdl7",
        "outputId": "19eb52e7-b506-4114-fe07-39dd399f8b8c"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC AUC: 1.0\n"
          ]
        }
      ]
    }
  ]
}